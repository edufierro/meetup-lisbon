{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple RNN example\n",
    "---\n",
    "\n",
    "In this notebook, we'll se a simple (perhaps the most simple) way of using RNNs... For text classification.\n",
    "\n",
    "For it, you'll need to download the data of IMDB sentiment classification: http://ai.stanford.edu/~amaas/data/sentiment/. This set is small enough for us to do it locally in our computers. It also provides a train and a test set. For our purpose, we'll use the test set as development/validation set. \n",
    "\n",
    "A lot of code of the preprocess Pipeline for IMDB dataset is from https://github.com/nyu-mll/DS-GA-1011-Fall2017/blob/master/hw1/HW01-student.ipynb (I recommend you doing this excersises and labs on your own! I took this class and it was extremely good. Syllable here: https://docs.google.com/document/d/1SIPSt4aeB3Lys9ztCp47Y4v68R6Awt8NBTlCObp2njg/edit). \n",
    "\n",
    "To tokenize, we're using the simple nltk word tokenizer (https://www.nltk.org/api/nltk.tokenize.html), but feel free to try moses or any other tokenizer. To use it, you'll probably have to download nltk's data. \n",
    "\n",
    "Creating this notebook, I came across someone who did something similar:\n",
    "* https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. File params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data/aclImdb/\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "TRAIN_SIZE = 23000 \n",
    "VALIDATION_SIZE = 2000\n",
    "TEST_SIZE = 2000 # I'm just loading 2000 out of the 25,000 to be able to work locally. Feel free to increase to 25k.\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# These can be treated as a hyperparm. \n",
    "VOCAB_SIZE = 20000 \n",
    "BATCH_SIZE = 40\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_DIM = 256\n",
    "EMBEDDING_DIM = 400\n",
    "MAX_LEN = 200\n",
    "\n",
    "# GPU use (I'll be asuming 1 device). This will be hard coded throughout the notebook: \n",
    "if torch.cuda.is_available():\n",
    "    USE_CUDA = True\n",
    "    DEFAULT_DEVICE = 'cuda:0'\n",
    "else:\n",
    "    DEFAULT_DEVICE = 'cpu'\n",
    "print(DEFAULT_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-process data\n",
    "\n",
    "Here, we're just loading the data into memory and we'll tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDatum():\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test datum\n",
    "    - self.raw_text\n",
    "    - self.label: 0 neg, 1 pos\n",
    "    - self.file_name: dir for this datum\n",
    "    - self.tokens: list of tokens\n",
    "    - self.token_idx: index of each token in the text\n",
    "    \"\"\"\n",
    "    def __init__(self, raw_text, label, file_name):\n",
    "        self.raw_text = raw_text\n",
    "        self.label = label\n",
    "        self.file_name = file_name\n",
    "        self.tokens = self._set_tokens(raw_text)\n",
    "        self.tokens_idx = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def _set_tokens(raw_text):\n",
    "        return word_tokenize(raw_text)\n",
    "        \n",
    "    def set_token_idx(self, token2idx, unk_token):\n",
    "        tokens_idx = []\n",
    "        for token in self.tokens:\n",
    "            token_idx = token2idx.get(token, token2idx[unk_token])\n",
    "            tokens_idx.append(token_idx)\n",
    "\n",
    "        self.tokens_idx = tokens_idx\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Function that cleans the string\n",
    "    \"\"\"\n",
    "    text = text.lower().replace(\"<br />\", \"\")\n",
    "    return text\n",
    "        \n",
    "    \n",
    "def read_file_as_datum(file_name, label):\n",
    "    \"\"\"\n",
    "    Function that reads a file \n",
    "    \"\"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        content = f.read()\n",
    "        content = preprocess_text(content)\n",
    "    return IMDBDatum(raw_text=content, label=label, file_name=file_name)\n",
    "\n",
    "\n",
    "def construct_dataset(dataset_dir, dataset_size, offset=0):\n",
    "    \"\"\"\n",
    "    Function that loads a dataset\n",
    "    \"\"\"\n",
    "    pos_dir = os.path.join(dataset_dir, \"pos\")\n",
    "    neg_dir = os.path.join(dataset_dir, \"neg\")\n",
    "    single_label_size = int(dataset_size / 2)\n",
    "    output = []\n",
    "    all_pos = os.listdir(pos_dir)\n",
    "    all_neg = os.listdir(neg_dir)\n",
    "    for i in range(offset, offset+single_label_size):\n",
    "        output.append(read_file_as_datum(os.path.join(pos_dir, all_pos[i]), 1))\n",
    "        output.append(read_file_as_datum(os.path.join(neg_dir, all_neg[i]), 0))\n",
    "    return output\n",
    "\n",
    "def filter_dataum_dataset(dataset, max_len):\n",
    "    new_output = []\n",
    "    removed_samples = 0\n",
    "    total_samples = len(dataset)\n",
    "    for sample in dataset:\n",
    "        if len(sample.tokens) > max_len:\n",
    "            removed_samples += 1\n",
    "            continue\n",
    "        new_output.append(sample)\n",
    "            \n",
    "    print('Removed {} samples, thats {}% of set'.format(removed_samples, 100*round(removed_samples/total_samples, 2)))\n",
    "    print('Total samples: {}'.format(len(new_output)))\n",
    "    return new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in train set...\n",
      "Removed 11291 samples, thats 49.0% of set\n",
      "Total samples: 11709\n",
      "Working in dev set...\n",
      "Removed 980 samples, thats 49.0% of set\n",
      "Total samples: 1020\n",
      "Working in test set...\n",
      "Removed 966 samples, thats 48.0% of set\n",
      "Total samples: 1034\n"
     ]
    }
   ],
   "source": [
    "print('Working in train set...')\n",
    "train_set = construct_dataset(train_dir, TRAIN_SIZE)\n",
    "train_set = filter_dataum_dataset(train_set, MAX_LEN)\n",
    "print('Working in dev set...')\n",
    "validation_set = construct_dataset(train_dir, VALIDATION_SIZE, offset=int(TRAIN_SIZE/2))\n",
    "validation_set = filter_dataum_dataset(validation_set, MAX_LEN)\n",
    "print('Working in test set...')\n",
    "test_set = construct_dataset(test_dir, TEST_SIZE)\n",
    "test_set = filter_dataum_dataset(test_set, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['broad', 'enough', 'for', 'you', '?', 'wait', 'till', 'you', 'see', 'this']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'film', 'well', '.', 'not', 'a', 'lot', 'of', 'fun', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].tokens[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"broad enough for you? wait till you see this heavy handedadaption of a little collegiate one act. what is shocking and wild incollege rarely holds up over time, and this is proof. to take on thecatholic church with broadside humor just isn't shocking orinteresting or funny, it's kind of boring. the performers are allgame, giving all they've got, but it's basically a play that doesn'topen up to film well. not a lot of fun.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "\n",
    "Decide what features we should use (maybe tokenization decision should be here?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token2idx(train_set, vocab_size=VOCAB_SIZE):\n",
    "    \"\"\"\n",
    "    Function that loads the train set and return a dict that maps tokens to index. \n",
    "    Indexes 0 and 1 are reserved for Padding and Unknown tokens respectively. \n",
    "    Currently hard coded. \n",
    "    \"\"\"\n",
    "    tokens_counter = Counter()\n",
    "    for datum in train_set:\n",
    "        for token in datum.tokens:\n",
    "            tokens_counter[token] += 1\n",
    "            \n",
    "    print('Number of unique tokens in train data: {}'.format(len(tokens_counter)))\n",
    "    print('Subsetting to: {}'.format(vocab_size))\n",
    "    top_k_tokens = tokens_counter.most_common(vocab_size) # This return a list of touples, not a Counter() object.\n",
    "    token2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for token_touples in top_k_tokens:\n",
    "        token = token_touples[0]\n",
    "        if token in token2idx.keys():\n",
    "            continue\n",
    "        \n",
    "        token2idx[token] = len(token2idx) + 1\n",
    "        \n",
    "    return token2idx\n",
    "\n",
    "def build_idx2tokens(token2idx):\n",
    "    \"\"\"\n",
    "    Function to build a dictionary that maps indexes to tokens from the reverse maps/\n",
    "    \"\"\"\n",
    "    return {v:k for k,v in token2idx.items()}\n",
    "\n",
    "def set_tokens_idx_in_datum(data_set, token2idx, unk_token='<UNK>'):\n",
    "    \"\"\"\n",
    "    Function to set the tokens in the list of datums.\n",
    "    \"\"\"\n",
    "    for datum in data_set:\n",
    "        datum.set_token_idx(token2idx, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in train data: 55344\n",
      "Subsetting to: 20000\n"
     ]
    }
   ],
   "source": [
    "token2idx = build_token2idx(train_set)\n",
    "idx2tokens = build_idx2tokens(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_tokens_idx_in_datum(train_set, token2idx)\n",
    "set_tokens_idx_in_datum(validation_set, token2idx)\n",
    "set_tokens_idx_in_datum(test_set, token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5185, 226, 19, 21, 61, 597, 1732, 21, 65, 13]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].tokens_idx[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broad\n",
      "enough\n",
      "for\n",
      "you\n",
      "?\n",
      "wait\n",
      "till\n",
      "you\n",
      "see\n",
      "this\n"
     ]
    }
   ],
   "source": [
    "# Sanity check (ALWAYS DO SANITY CHECKS!!! It's so easy to screw up in this steps)\n",
    "for x in train_set[0].tokens_idx[0:10]:\n",
    "    print(idx2tokens[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " 'the': 3,\n",
       " '.': 4,\n",
       " ',': 5,\n",
       " 'a': 6,\n",
       " 'and': 7,\n",
       " 'of': 8,\n",
       " 'to': 9,\n",
       " 'is': 10,\n",
       " 'i': 11,\n",
       " 'it': 12,\n",
       " 'this': 13,\n",
       " 'in': 14,\n",
       " 'that': 15,\n",
       " 'movie': 16,\n",
       " 'was': 17,\n",
       " \"'s\": 18,\n",
       " 'for': 19,\n",
       " 'but': 20,\n",
       " 'you': 21,\n",
       " 'film': 22,\n",
       " 'with': 23,\n",
       " 'as': 24,\n",
       " \"n't\": 25,\n",
       " '!': 26,\n",
       " 'not': 27,\n",
       " 'have': 28,\n",
       " 'on': 29,\n",
       " 'are': 30,\n",
       " '``': 31,\n",
       " \"''\": 32,\n",
       " ')': 33,\n",
       " 'one': 34,\n",
       " 'be': 35,\n",
       " '(': 36,\n",
       " 'all': 37,\n",
       " 'at': 38,\n",
       " 'like': 39,\n",
       " 'so': 40,\n",
       " 'they': 41,\n",
       " 'an': 42,\n",
       " 'do': 43,\n",
       " 'good': 44,\n",
       " 'his': 45,\n",
       " 'just': 46,\n",
       " 'if': 47,\n",
       " 'from': 48,\n",
       " 'he': 49,\n",
       " 'who': 50,\n",
       " 'by': 51,\n",
       " 'about': 52,\n",
       " 'there': 53,\n",
       " 'out': 54,\n",
       " 'very': 55,\n",
       " 'or': 56,\n",
       " '...': 57,\n",
       " 'my': 58,\n",
       " 'what': 59,\n",
       " 'has': 60,\n",
       " '?': 61,\n",
       " 'some': 62,\n",
       " 'would': 63,\n",
       " 'really': 64,\n",
       " 'see': 65,\n",
       " 'more': 66,\n",
       " 'story': 67,\n",
       " 'time': 68,\n",
       " 'can': 69,\n",
       " 'even': 70,\n",
       " 'me': 71,\n",
       " 'were': 72,\n",
       " 'no': 73,\n",
       " 'when': 74,\n",
       " 'great': 75,\n",
       " 'bad': 76,\n",
       " 'did': 77,\n",
       " 'only': 78,\n",
       " 'her': 79,\n",
       " 'had': 80,\n",
       " 'up': 81,\n",
       " 'could': 82,\n",
       " 'will': 83,\n",
       " 'watch': 84,\n",
       " 'well': 85,\n",
       " 'people': 86,\n",
       " 'movies': 87,\n",
       " 'much': 88,\n",
       " 'she': 89,\n",
       " 'get': 90,\n",
       " 'than': 91,\n",
       " 'acting': 92,\n",
       " 'how': 93,\n",
       " 'which': 94,\n",
       " 'made': 95,\n",
       " 'their': 96,\n",
       " 'been': 97,\n",
       " 'seen': 98,\n",
       " 'think': 99,\n",
       " '-': 100,\n",
       " 'does': 101,\n",
       " 'we': 102,\n",
       " 'first': 103,\n",
       " 'because': 104,\n",
       " ':': 105,\n",
       " 'make': 106,\n",
       " 'most': 107,\n",
       " 'other': 108,\n",
       " 'ever': 109,\n",
       " 'plot': 110,\n",
       " 'its': 111,\n",
       " 'also': 112,\n",
       " 'too': 113,\n",
       " 'way': 114,\n",
       " 'best': 115,\n",
       " 'any': 116,\n",
       " 'your': 117,\n",
       " 'love': 118,\n",
       " 'characters': 119,\n",
       " 'into': 120,\n",
       " 'show': 121,\n",
       " \"'\": 122,\n",
       " 'then': 123,\n",
       " 'after': 124,\n",
       " 'better': 125,\n",
       " 'never': 126,\n",
       " 'should': 127,\n",
       " 'many': 128,\n",
       " 'watching': 129,\n",
       " 'funny': 130,\n",
       " 'films': 131,\n",
       " 'little': 132,\n",
       " 'life': 133,\n",
       " 'know': 134,\n",
       " 'them': 135,\n",
       " 'say': 136,\n",
       " \"'ve\": 137,\n",
       " 'actors': 138,\n",
       " 'character': 139,\n",
       " 'two': 140,\n",
       " 'still': 141,\n",
       " 'end': 142,\n",
       " 'scenes': 143,\n",
       " 'over': 144,\n",
       " 'where': 145,\n",
       " \"'m\": 146,\n",
       " 'being': 147,\n",
       " 'go': 148,\n",
       " 'him': 149,\n",
       " 'why': 150,\n",
       " 'such': 151,\n",
       " 'real': 152,\n",
       " 'through': 153,\n",
       " 'thing': 154,\n",
       " ';': 155,\n",
       " 'something': 156,\n",
       " 'off': 157,\n",
       " 'saw': 158,\n",
       " 'cast': 159,\n",
       " 'want': 160,\n",
       " 'these': 161,\n",
       " 'those': 162,\n",
       " 'nothing': 163,\n",
       " 'years': 164,\n",
       " 'scene': 165,\n",
       " 'here': 166,\n",
       " 'lot': 167,\n",
       " 'back': 168,\n",
       " 'makes': 169,\n",
       " 'got': 170,\n",
       " 'look': 171,\n",
       " 'every': 172,\n",
       " 'thought': 173,\n",
       " 'find': 174,\n",
       " 'old': 175,\n",
       " 'pretty': 176,\n",
       " 'man': 177,\n",
       " 'again': 178,\n",
       " 'comedy': 179,\n",
       " 'though': 180,\n",
       " 'worst': 181,\n",
       " 'ca': 182,\n",
       " 'actually': 183,\n",
       " 'few': 184,\n",
       " 'interesting': 185,\n",
       " 'now': 186,\n",
       " '--': 187,\n",
       " 'part': 188,\n",
       " 'another': 189,\n",
       " 'same': 190,\n",
       " 'work': 191,\n",
       " 'going': 192,\n",
       " 'while': 193,\n",
       " 'give': 194,\n",
       " 'must': 195,\n",
       " 'director': 196,\n",
       " 'before': 197,\n",
       " 'script': 198,\n",
       " 'series': 199,\n",
       " 'quite': 200,\n",
       " 'original': 201,\n",
       " 'new': 202,\n",
       " 'am': 203,\n",
       " 'big': 204,\n",
       " 'times': 205,\n",
       " 'music': 206,\n",
       " 'world': 207,\n",
       " 'family': 208,\n",
       " 'worth': 209,\n",
       " 'always': 210,\n",
       " 'fun': 211,\n",
       " 'watched': 212,\n",
       " 'things': 213,\n",
       " 'anyone': 214,\n",
       " 'action': 215,\n",
       " 'without': 216,\n",
       " 'done': 217,\n",
       " 'whole': 218,\n",
       " \"'re\": 219,\n",
       " 'us': 220,\n",
       " 'take': 221,\n",
       " 'long': 222,\n",
       " 'tv': 223,\n",
       " 'young': 224,\n",
       " 'may': 225,\n",
       " 'enough': 226,\n",
       " 'horror': 227,\n",
       " 'minutes': 228,\n",
       " 'anything': 229,\n",
       " \"'ll\": 230,\n",
       " 'found': 231,\n",
       " 'feel': 232,\n",
       " 'down': 233,\n",
       " 'come': 234,\n",
       " 'bit': 235,\n",
       " 'excellent': 236,\n",
       " 'believe': 237,\n",
       " 'however': 238,\n",
       " 'recommend': 239,\n",
       " 'performance': 240,\n",
       " 'dvd': 241,\n",
       " 'around': 242,\n",
       " 'least': 243,\n",
       " 'between': 244,\n",
       " 'role': 245,\n",
       " 'probably': 246,\n",
       " '10': 247,\n",
       " 'especially': 248,\n",
       " 'almost': 249,\n",
       " 'book': 250,\n",
       " 'right': 251,\n",
       " 'money': 252,\n",
       " 'since': 253,\n",
       " 'far': 254,\n",
       " 'gets': 255,\n",
       " 'both': 256,\n",
       " 'enjoy': 257,\n",
       " 'seems': 258,\n",
       " 'shows': 259,\n",
       " 'fact': 260,\n",
       " 'guy': 261,\n",
       " 'sure': 262,\n",
       " 'last': 263,\n",
       " 'waste': 264,\n",
       " 'ending': 265,\n",
       " 'fan': 266,\n",
       " 'hard': 267,\n",
       " 'although': 268,\n",
       " 'own': 269,\n",
       " 'everyone': 270,\n",
       " 'kind': 271,\n",
       " 'boring': 272,\n",
       " 'girl': 273,\n",
       " '&': 274,\n",
       " 'might': 275,\n",
       " 'wonderful': 276,\n",
       " 'put': 277,\n",
       " 'point': 278,\n",
       " 'away': 279,\n",
       " 'seeing': 280,\n",
       " 'effects': 281,\n",
       " 'actor': 282,\n",
       " 'looking': 283,\n",
       " '2': 284,\n",
       " 'job': 285,\n",
       " 'awful': 286,\n",
       " 'terrible': 287,\n",
       " 'maybe': 288,\n",
       " 'making': 289,\n",
       " 'special': 290,\n",
       " 'beautiful': 291,\n",
       " 'day': 292,\n",
       " 'read': 293,\n",
       " 'someone': 294,\n",
       " 'liked': 295,\n",
       " 'version': 296,\n",
       " 'loved': 297,\n",
       " 'true': 298,\n",
       " 'trying': 299,\n",
       " 'plays': 300,\n",
       " \"'d\": 301,\n",
       " 'nice': 302,\n",
       " 'played': 303,\n",
       " 'together': 304,\n",
       " 'poor': 305,\n",
       " 'yet': 306,\n",
       " 'remember': 307,\n",
       " 'rather': 308,\n",
       " 'everything': 309,\n",
       " 'sense': 310,\n",
       " 'looks': 311,\n",
       " 'different': 312,\n",
       " 'our': 313,\n",
       " 'line': 314,\n",
       " 'kids': 315,\n",
       " 'idea': 316,\n",
       " 'classic': 317,\n",
       " 'john': 318,\n",
       " 'laugh': 319,\n",
       " 'each': 320,\n",
       " 'stupid': 321,\n",
       " 'video': 322,\n",
       " 'definitely': 323,\n",
       " 'main': 324,\n",
       " 'once': 325,\n",
       " 'left': 326,\n",
       " 'reason': 327,\n",
       " 'completely': 328,\n",
       " 'else': 329,\n",
       " 'comes': 330,\n",
       " 'woman': 331,\n",
       " 'absolutely': 332,\n",
       " 'high': 333,\n",
       " 'enjoyed': 334,\n",
       " 'having': 335,\n",
       " 'play': 336,\n",
       " 'night': 337,\n",
       " 'hope': 338,\n",
       " 'hollywood': 339,\n",
       " 'truly': 340,\n",
       " 'let': 341,\n",
       " 'year': 342,\n",
       " 'performances': 343,\n",
       " 'need': 344,\n",
       " 'episode': 345,\n",
       " 'goes': 346,\n",
       " 'budget': 347,\n",
       " 'screen': 348,\n",
       " 'american': 349,\n",
       " 'set': 350,\n",
       " 'understand': 351,\n",
       " 'instead': 352,\n",
       " 'piece': 353,\n",
       " 'said': 354,\n",
       " 'half': 355,\n",
       " 'try': 356,\n",
       " 'place': 357,\n",
       " 'short': 358,\n",
       " 'entertaining': 359,\n",
       " 'seem': 360,\n",
       " 'friends': 361,\n",
       " 'simply': 362,\n",
       " 'takes': 363,\n",
       " 'lines': 364,\n",
       " 'audience': 365,\n",
       " 'worse': 366,\n",
       " 'second': 367,\n",
       " 'black': 368,\n",
       " 'course': 369,\n",
       " 'three': 370,\n",
       " 'wrong': 371,\n",
       " 'tell': 372,\n",
       " 'production': 373,\n",
       " 'shot': 374,\n",
       " 'horrible': 375,\n",
       " 'mind': 376,\n",
       " 'amazing': 377,\n",
       " 'rest': 378,\n",
       " 'humor': 379,\n",
       " 'favorite': 380,\n",
       " 'used': 381,\n",
       " 'came': 382,\n",
       " 'children': 383,\n",
       " 'perfect': 384,\n",
       " 'felt': 385,\n",
       " 'stars': 386,\n",
       " 'low': 387,\n",
       " 'during': 388,\n",
       " 'full': 389,\n",
       " 'keep': 390,\n",
       " 'either': 391,\n",
       " 'mean': 392,\n",
       " 'dialogue': 393,\n",
       " 'women': 394,\n",
       " 'went': 395,\n",
       " 'star': 396,\n",
       " 'use': 397,\n",
       " 'please': 398,\n",
       " 'top': 399,\n",
       " 'gave': 400,\n",
       " 'along': 401,\n",
       " 'drama': 402,\n",
       " 'next': 403,\n",
       " 'help': 404,\n",
       " 'wife': 405,\n",
       " 'moments': 406,\n",
       " 'war': 407,\n",
       " 'written': 408,\n",
       " 'until': 409,\n",
       " 'couple': 410,\n",
       " 'start': 411,\n",
       " 'less': 412,\n",
       " 'person': 413,\n",
       " 'small': 414,\n",
       " 'gives': 415,\n",
       " 'sex': 416,\n",
       " 'school': 417,\n",
       " '3': 418,\n",
       " 'highly': 419,\n",
       " 'totally': 420,\n",
       " 'home': 421,\n",
       " 'flick': 422,\n",
       " 'seemed': 423,\n",
       " 'direction': 424,\n",
       " 'camera': 425,\n",
       " 'parts': 426,\n",
       " 'writing': 427,\n",
       " 'beginning': 428,\n",
       " 'guess': 429,\n",
       " 'entire': 430,\n",
       " 'quality': 431,\n",
       " 'given': 432,\n",
       " 'lost': 433,\n",
       " 'cinema': 434,\n",
       " 'fans': 435,\n",
       " 'house': 436,\n",
       " 'others': 437,\n",
       " 'brilliant': 438,\n",
       " 'name': 439,\n",
       " 'wanted': 440,\n",
       " 'crap': 441,\n",
       " 'live': 442,\n",
       " 'supposed': 443,\n",
       " 'getting': 444,\n",
       " 'playing': 445,\n",
       " 'wish': 446,\n",
       " 'picture': 447,\n",
       " 'early': 448,\n",
       " 'yes': 449,\n",
       " 'father': 450,\n",
       " 'death': 451,\n",
       " 'history': 452,\n",
       " '1': 453,\n",
       " 'disappointed': 454,\n",
       " 'care': 455,\n",
       " 'dead': 456,\n",
       " 'fine': 457,\n",
       " 'title': 458,\n",
       " 'himself': 459,\n",
       " 'expect': 460,\n",
       " 'yourself': 461,\n",
       " 'based': 462,\n",
       " 'several': 463,\n",
       " 'dark': 464,\n",
       " 'certainly': 465,\n",
       " 'lives': 466,\n",
       " 'human': 467,\n",
       " 'stuff': 468,\n",
       " 'men': 469,\n",
       " 'thinking': 470,\n",
       " 'perhaps': 471,\n",
       " 'jokes': 472,\n",
       " 'itself': 473,\n",
       " 'hilarious': 474,\n",
       " 'myself': 475,\n",
       " 'oh': 476,\n",
       " 'slow': 477,\n",
       " 'rent': 478,\n",
       " 'lead': 479,\n",
       " 'today': 480,\n",
       " 'later': 481,\n",
       " 'killer': 482,\n",
       " 'overall': 483,\n",
       " 'predictable': 484,\n",
       " 'kid': 485,\n",
       " 'wonder': 486,\n",
       " 'guys': 487,\n",
       " 'throughout': 488,\n",
       " 'wo': 489,\n",
       " 'boy': 490,\n",
       " 'hour': 491,\n",
       " 'doing': 492,\n",
       " 'sad': 493,\n",
       " 'style': 494,\n",
       " 'decent': 495,\n",
       " 'face': 496,\n",
       " 'child': 497,\n",
       " 'scary': 498,\n",
       " 'extremely': 499,\n",
       " 'days': 500,\n",
       " 'friend': 501,\n",
       " 'viewer': 502,\n",
       " 'art': 503,\n",
       " 'feeling': 504,\n",
       " 'hours': 505,\n",
       " 'except': 506,\n",
       " 'heart': 507,\n",
       " 'game': 508,\n",
       " 'case': 509,\n",
       " 'ok': 510,\n",
       " 'heard': 511,\n",
       " 'save': 512,\n",
       " 'sort': 513,\n",
       " 'head': 514,\n",
       " 'white': 515,\n",
       " 'documentary': 516,\n",
       " 'become': 517,\n",
       " 'often': 518,\n",
       " 'opinion': 519,\n",
       " 'sound': 520,\n",
       " 'actress': 521,\n",
       " 'sometimes': 522,\n",
       " 'able': 523,\n",
       " 'unfortunately': 524,\n",
       " 'ago': 525,\n",
       " 'michael': 526,\n",
       " 'complete': 527,\n",
       " 'ridiculous': 528,\n",
       " 'episodes': 529,\n",
       " 'storyline': 530,\n",
       " 'chance': 531,\n",
       " 'looked': 532,\n",
       " 'finally': 533,\n",
       " 'involved': 534,\n",
       " 'act': 535,\n",
       " 'eyes': 536,\n",
       " 'cool': 537,\n",
       " 'avoid': 538,\n",
       " 'surprised': 539,\n",
       " 'side': 540,\n",
       " 'happened': 541,\n",
       " 'buy': 542,\n",
       " 'already': 543,\n",
       " 'directed': 544,\n",
       " 'simple': 545,\n",
       " 'problem': 546,\n",
       " 'run': 547,\n",
       " 'obviously': 548,\n",
       " 'seriously': 549,\n",
       " '5': 550,\n",
       " 'roles': 551,\n",
       " 'cinematography': 552,\n",
       " 'turn': 553,\n",
       " 'turns': 554,\n",
       " 'late': 555,\n",
       " 'enjoyable': 556,\n",
       " 'etc': 557,\n",
       " 'annoying': 558,\n",
       " 'example': 559,\n",
       " 'stories': 560,\n",
       " 'genre': 561,\n",
       " 'happen': 562,\n",
       " 'girls': 563,\n",
       " 'took': 564,\n",
       " 'talent': 565,\n",
       " 'sorry': 566,\n",
       " 'called': 567,\n",
       " 'comments': 568,\n",
       " 'mother': 569,\n",
       " 'attempt': 570,\n",
       " 'strong': 571,\n",
       " 'type': 572,\n",
       " 'car': 573,\n",
       " 'theater': 574,\n",
       " 'usual': 575,\n",
       " 'entertainment': 576,\n",
       " 'works': 577,\n",
       " 'serious': 578,\n",
       " 'voice': 579,\n",
       " '4': 580,\n",
       " 'rating': 581,\n",
       " 'son': 582,\n",
       " 'started': 583,\n",
       " 'past': 584,\n",
       " 'stop': 585,\n",
       " 'alone': 586,\n",
       " 'god': 587,\n",
       " 'under': 588,\n",
       " 'silly': 589,\n",
       " 'lots': 590,\n",
       " 'matter': 591,\n",
       " 'evil': 592,\n",
       " 'song': 593,\n",
       " 'television': 594,\n",
       " 'age': 595,\n",
       " 'close': 596,\n",
       " 'wait': 597,\n",
       " 'dull': 598,\n",
       " 'strange': 599,\n",
       " 'despite': 600,\n",
       " 'fight': 601,\n",
       " 'shown': 602,\n",
       " 'released': 603,\n",
       " 'songs': 604,\n",
       " 'becomes': 605,\n",
       " 'supporting': 606,\n",
       " 'leave': 607,\n",
       " 'mr.': 608,\n",
       " 'behind': 609,\n",
       " 'view': 610,\n",
       " 'experience': 611,\n",
       " 'wants': 612,\n",
       " 'miss': 613,\n",
       " 'fantastic': 614,\n",
       " 'told': 615,\n",
       " 'themselves': 616,\n",
       " 'usually': 617,\n",
       " 'lame': 618,\n",
       " 'starts': 619,\n",
       " 'coming': 620,\n",
       " 'ten': 621,\n",
       " 'soundtrack': 622,\n",
       " 'against': 623,\n",
       " 'romantic': 624,\n",
       " 'violence': 625,\n",
       " 'reality': 626,\n",
       " 'superb': 627,\n",
       " 'final': 628,\n",
       " 'turned': 629,\n",
       " 'cheap': 630,\n",
       " 'james': 631,\n",
       " 'interest': 632,\n",
       " 'writer': 633,\n",
       " 'anyway': 634,\n",
       " 'check': 635,\n",
       " 'gore': 636,\n",
       " 'hell': 637,\n",
       " 'living': 638,\n",
       " 'moving': 639,\n",
       " 'bring': 640,\n",
       " 'season': 641,\n",
       " 'richard': 642,\n",
       " 'tries': 643,\n",
       " 'weak': 644,\n",
       " 'ones': 645,\n",
       " 'animation': 646,\n",
       " 'happens': 647,\n",
       " 'future': 648,\n",
       " 'thriller': 649,\n",
       " 'novel': 650,\n",
       " 'murder': 651,\n",
       " 'sit': 652,\n",
       " 'comment': 653,\n",
       " 'daughter': 654,\n",
       " 'words': 655,\n",
       " 'sequel': 656,\n",
       " 'shame': 657,\n",
       " 'attention': 658,\n",
       " 'particularly': 659,\n",
       " 'unless': 660,\n",
       " 'message': 661,\n",
       " 'group': 662,\n",
       " 'change': 663,\n",
       " 'basically': 664,\n",
       " 'happy': 665,\n",
       " 'city': 666,\n",
       " 'kill': 667,\n",
       " 'huge': 668,\n",
       " 'realistic': 669,\n",
       " 'none': 670,\n",
       " 'robert': 671,\n",
       " 'taken': 672,\n",
       " 'lack': 673,\n",
       " 'word': 674,\n",
       " 'across': 675,\n",
       " 'knew': 676,\n",
       " 'poorly': 677,\n",
       " 'filmed': 678,\n",
       " 'call': 679,\n",
       " 'town': 680,\n",
       " 'viewing': 681,\n",
       " 'shots': 682,\n",
       " 'soon': 683,\n",
       " 'five': 684,\n",
       " 'mystery': 685,\n",
       " 'directing': 686,\n",
       " 'acted': 687,\n",
       " 'british': 688,\n",
       " 'known': 689,\n",
       " 'english': 690,\n",
       " 'brother': 691,\n",
       " 'somewhat': 692,\n",
       " 'fast': 693,\n",
       " 'dialog': 694,\n",
       " 'major': 695,\n",
       " 'add': 696,\n",
       " 'country': 697,\n",
       " 'laughs': 698,\n",
       " 'wasted': 699,\n",
       " 'including': 700,\n",
       " 'david': 701,\n",
       " 'forward': 702,\n",
       " 'greatest': 703,\n",
       " 'blood': 704,\n",
       " 'talking': 705,\n",
       " 'obvious': 706,\n",
       " 'level': 707,\n",
       " 'musical': 708,\n",
       " 'moment': 709,\n",
       " 'effort': 710,\n",
       " 'single': 711,\n",
       " 'kept': 712,\n",
       " 'hate': 713,\n",
       " 'beyond': 714,\n",
       " 'hit': 715,\n",
       " 'possible': 716,\n",
       " 'tom': 717,\n",
       " 'peter': 718,\n",
       " 'needs': 719,\n",
       " 'career': 720,\n",
       " 'typical': 721,\n",
       " 'cute': 722,\n",
       " 'easy': 723,\n",
       " 'female': 724,\n",
       " '$': 725,\n",
       " 'disney': 726,\n",
       " 'comic': 727,\n",
       " 'score': 728,\n",
       " 'tried': 729,\n",
       " 'order': 730,\n",
       " 'expected': 731,\n",
       " 'difficult': 732,\n",
       " 'average': 733,\n",
       " 'cheesy': 734,\n",
       " 'relationship': 735,\n",
       " 'bored': 736,\n",
       " 'tale': 737,\n",
       " 'editing': 738,\n",
       " 'easily': 739,\n",
       " 'reviews': 740,\n",
       " 'husband': 741,\n",
       " 'ends': 742,\n",
       " 'modern': 743,\n",
       " 'george': 744,\n",
       " 'taking': 745,\n",
       " 'awesome': 746,\n",
       " 'weird': 747,\n",
       " 'opening': 748,\n",
       " 'exactly': 749,\n",
       " 'running': 750,\n",
       " 'saying': 751,\n",
       " 'showing': 752,\n",
       " 'class': 753,\n",
       " 'says': 754,\n",
       " 'copy': 755,\n",
       " 'eye': 756,\n",
       " 'write': 757,\n",
       " 'fall': 758,\n",
       " 'interested': 759,\n",
       " 'police': 760,\n",
       " 'bunch': 761,\n",
       " 'hand': 762,\n",
       " 'recommended': 763,\n",
       " 'jack': 764,\n",
       " 'expecting': 765,\n",
       " 'nearly': 766,\n",
       " 'cut': 767,\n",
       " 'light': 768,\n",
       " 'hot': 769,\n",
       " 'imagine': 770,\n",
       " 'king': 771,\n",
       " 'mention': 772,\n",
       " 'due': 773,\n",
       " 'killed': 774,\n",
       " 'move': 775,\n",
       " 'earth': 776,\n",
       " 'talk': 777,\n",
       " 'power': 778,\n",
       " 'possibly': 779,\n",
       " 'space': 780,\n",
       " 'romance': 781,\n",
       " 'believable': 782,\n",
       " 'suspense': 783,\n",
       " 'oscar': 784,\n",
       " 'similar': 785,\n",
       " 'number': 786,\n",
       " 'plain': 787,\n",
       " 'sets': 788,\n",
       " 'middle': 789,\n",
       " 'surprise': 790,\n",
       " 'forget': 791,\n",
       " 'stay': 792,\n",
       " 'laughing': 793,\n",
       " 'deserves': 794,\n",
       " 'french': 795,\n",
       " 'viewers': 796,\n",
       " 'mostly': 797,\n",
       " 'rock': 798,\n",
       " 'keeps': 799,\n",
       " 'reading': 800,\n",
       " 'premise': 801,\n",
       " 'okay': 802,\n",
       " 'review': 803,\n",
       " 'hero': 804,\n",
       " 'gone': 805,\n",
       " 'ways': 806,\n",
       " 'release': 807,\n",
       " 'joke': 808,\n",
       " 'badly': 809,\n",
       " 'actual': 810,\n",
       " 'falls': 811,\n",
       " 'local': 812,\n",
       " 'leads': 813,\n",
       " 'giving': 814,\n",
       " 'incredible': 815,\n",
       " 'total': 816,\n",
       " 'body': 817,\n",
       " 'doubt': 818,\n",
       " 'parents': 819,\n",
       " 'decided': 820,\n",
       " 'within': 821,\n",
       " 'problems': 822,\n",
       " 'important': 823,\n",
       " 'paul': 824,\n",
       " 'glad': 825,\n",
       " 'dumb': 826,\n",
       " 'became': 827,\n",
       " 'laughed': 828,\n",
       " 'subject': 829,\n",
       " 'points': 830,\n",
       " 'events': 831,\n",
       " 'missed': 832,\n",
       " 'period': 833,\n",
       " 'incredibly': 834,\n",
       " 'four': 835,\n",
       " 'room': 836,\n",
       " 'imdb': 837,\n",
       " 'mess': 838,\n",
       " 'above': 839,\n",
       " 'follow': 840,\n",
       " 'learn': 841,\n",
       " 'needed': 842,\n",
       " 'rate': 843,\n",
       " 'agree': 844,\n",
       " 'brought': 845,\n",
       " 'admit': 846,\n",
       " 'b': 847,\n",
       " 'certain': 848,\n",
       " 'using': 849,\n",
       " 'trash': 850,\n",
       " 'material': 851,\n",
       " 'bill': 852,\n",
       " 'credits': 853,\n",
       " 'free': 854,\n",
       " 'deep': 855,\n",
       " 'nor': 856,\n",
       " 'pathetic': 857,\n",
       " 'appreciate': 858,\n",
       " '20': 859,\n",
       " 'feature': 860,\n",
       " 'remake': 861,\n",
       " 'form': 862,\n",
       " 'garbage': 863,\n",
       " 'unbelievable': 864,\n",
       " 'whether': 865,\n",
       " 'straight': 866,\n",
       " 'unique': 867,\n",
       " 'masterpiece': 868,\n",
       " 'general': 869,\n",
       " 'writers': 870,\n",
       " 'perfectly': 871,\n",
       " 'talented': 872,\n",
       " 'realize': 873,\n",
       " 'clearly': 874,\n",
       " 'atmosphere': 875,\n",
       " 'directors': 876,\n",
       " 'leaves': 877,\n",
       " 'theme': 878,\n",
       " 'lee': 879,\n",
       " 'western': 880,\n",
       " 'potential': 881,\n",
       " 'bought': 882,\n",
       " 'elements': 883,\n",
       " 'lady': 884,\n",
       " 'sci-fi': 885,\n",
       " 'hear': 886,\n",
       " '90': 887,\n",
       " 'famous': 888,\n",
       " 'cartoon': 889,\n",
       " 'pure': 890,\n",
       " 'near': 891,\n",
       " 'stand': 892,\n",
       " 'nudity': 893,\n",
       " 'york': 894,\n",
       " 'showed': 895,\n",
       " 'spent': 896,\n",
       " 'positive': 897,\n",
       " 'dance': 898,\n",
       " 'fairly': 899,\n",
       " 'store': 900,\n",
       " 'upon': 901,\n",
       " 'knows': 902,\n",
       " 'screenplay': 903,\n",
       " 'sister': 904,\n",
       " 'recently': 905,\n",
       " 'adult': 906,\n",
       " 'value': 907,\n",
       " 'hardly': 908,\n",
       " 'pay': 909,\n",
       " 'sweet': 910,\n",
       " 'funniest': 911,\n",
       " 'memorable': 912,\n",
       " 'ended': 913,\n",
       " 'japanese': 914,\n",
       " 'feels': 915,\n",
       " 'among': 916,\n",
       " 'casting': 917,\n",
       " 'list': 918,\n",
       " 'humour': 919,\n",
       " 'development': 920,\n",
       " 'clear': 921,\n",
       " 'minute': 922,\n",
       " 'otherwise': 923,\n",
       " 'truth': 924,\n",
       " 'plus': 925,\n",
       " 'ask': 926,\n",
       " 'gay': 927,\n",
       " 'means': 928,\n",
       " 'tells': 929,\n",
       " 'caught': 930,\n",
       " 'particular': 931,\n",
       " 'outstanding': 932,\n",
       " 'amusing': 933,\n",
       " 'america': 934,\n",
       " 'christmas': 935,\n",
       " 'older': 936,\n",
       " '30': 937,\n",
       " 'bother': 938,\n",
       " 'creepy': 939,\n",
       " 'pointless': 940,\n",
       " 'dancing': 941,\n",
       " 'fantasy': 942,\n",
       " 'apart': 943,\n",
       " 'red': 944,\n",
       " 'society': 945,\n",
       " 'public': 946,\n",
       " 'air': 947,\n",
       " 'twist': 948,\n",
       " 'apparently': 949,\n",
       " 'crime': 950,\n",
       " 'scott': 951,\n",
       " 'die': 952,\n",
       " '7': 953,\n",
       " 'solid': 954,\n",
       " 'clever': 955,\n",
       " 'waiting': 956,\n",
       " 'de': 957,\n",
       " 'meant': 958,\n",
       " 'college': 959,\n",
       " 'crazy': 960,\n",
       " 'thats': 961,\n",
       " 'fails': 962,\n",
       " 'beauty': 963,\n",
       " 'features': 964,\n",
       " 'create': 965,\n",
       " 'sexual': 966,\n",
       " 'flat': 967,\n",
       " 'background': 968,\n",
       " '80': 969,\n",
       " 'somehow': 970,\n",
       " '15': 971,\n",
       " 'plenty': 972,\n",
       " 'cry': 973,\n",
       " 'wow': 974,\n",
       " 'rated': 975,\n",
       " 'laughable': 976,\n",
       " 'festival': 977,\n",
       " 'channel': 978,\n",
       " 'adventure': 979,\n",
       " 'footage': 980,\n",
       " 'figure': 981,\n",
       " 'mark': 982,\n",
       " 'situations': 983,\n",
       " 'baby': 984,\n",
       " 'appears': 985,\n",
       " 'brings': 986,\n",
       " 'box': 987,\n",
       " 'likes': 988,\n",
       " 'spend': 989,\n",
       " 'filled': 990,\n",
       " 'william': 991,\n",
       " '8': 992,\n",
       " 'team': 993,\n",
       " 'begin': 994,\n",
       " 'compared': 995,\n",
       " 'rented': 996,\n",
       " 'disappointment': 997,\n",
       " 'costumes': 998,\n",
       " 'scenery': 999,\n",
       " 'fighting': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check (ALWAYS DO SANITY CHECKS!!! It's so easy to screw up in this steps)\n",
    "#idx2tokens\n",
    "# token2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pytorch pipeline\n",
    "\n",
    "Build the pytorch pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    See the documentation of pytorch's dataloaders here: \n",
    "    https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDBDatum\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        returns (tokens idxs list, len_of_sequence), label\n",
    "        \"\"\"\n",
    "        tokens_idx, label = self.data_list[key].tokens_idx, self.data_list[key].label\n",
    "        return (tokens_idx, len(tokens_idx)), label\n",
    "    \n",
    "\n",
    "\n",
    "def imdb_collate_and_force_len(batch, max_length=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length. \n",
    "    \n",
    "    PAD token is hard coded (0).\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    \n",
    "    # Padd the sequence (using numpy)\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[1])\n",
    "        length_list.append(datum[0][1])\n",
    "        padded_vec = np.pad(np.array(datum[0][0]), \n",
    "                            pad_width=((0,max_length-datum[0][1])), \n",
    "                            mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)).to(DEFAULT_DEVICE), torch.LongTensor(length_list).to(DEFAULT_DEVICE), torch.LongTensor(label_list).to(DEFAULT_DEVICE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an training sample: ([5185, 226, 19, 21, 61, 597, 1732, 21, 65, 13, 1323, 1, 8, 6, 132, 1, 34, 535, 4, 59, 10, 1469, 7, 1351, 1, 1499, 1541, 81, 144, 68, 5, 7, 13, 10, 2816, 4, 9, 221, 29, 1, 1779, 23, 1, 379, 46, 10, 25, 1469, 1, 56, 130, 5, 12, 18, 271, 8, 272, 4, 3, 2958, 30, 1, 5, 814, 37, 41, 137, 170, 5, 20, 12, 18, 664, 6, 336, 15, 1, 81, 9, 22, 85, 4, 27, 6, 167, 8, 211, 4], 88)\n",
      "This is a label: 0\n"
     ]
    }
   ],
   "source": [
    "# consturct datasets\n",
    "imdb_train = IMDBDataset(train_set)\n",
    "imdb_validation = IMDBDataset(validation_set)\n",
    "imdb_test = IMDBDataset(test_set)    \n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(dataset=imdb_train, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_collate_and_force_len,\n",
    "                                           shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=imdb_validation, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=imdb_collate_and_force_len,\n",
    "                                           shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=imdb_test, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_collate_and_force_len,\n",
    "                                           shuffle=False)\n",
    "\n",
    "print(\"This is an training sample: {0}\".format(imdb_train[0][0]))\n",
    "print(\"This is a label: {0}\".format(imdb_train[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple RNN model!!\n",
    "\n",
    "Build our very own RNN model!\n",
    "\n",
    "Check `models.py` to see specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OurAwsomeLSTMWithTwoLayers(\n",
       "  (embedding): Embedding(20003, 400, padding_idx=0)\n",
       "  (LSTM): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (linear): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = OurAwsomeRNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, DEFAULT_DEVICE)\n",
    "model = OurAwsomeLSTMWithTwoLayers(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, DEFAULT_DEVICE)\n",
    "# model = OurAwsomeRNNWithAllConnections(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, DEFAULT_DEVICE, MAX_LEN)\n",
    "model.to(DEFAULT_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our loss function\n",
    "# See all of pytorch loss functions: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define our optimizer:\n",
    "# See all of pytorch's optimizers: https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch = Variable(data)\n",
    "        outputs = model(data)\n",
    "        predicted = (outputs.data > 0.5).long().view(-1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().cpu().numpy()\n",
    "    model.train()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/20], Step: [80/292], Loss: 0.6971672177314758, Train Acc: 50.01281065846784, Validation Acc:49.31372549019608\n",
      "Epoch: [1/20], Step: [160/292], Loss: 0.6984521150588989, Train Acc: 50.02989153642497, Validation Acc:49.21568627450981\n",
      "Epoch: [1/20], Step: [240/292], Loss: 0.6994984745979309, Train Acc: 50.12383636518917, Validation Acc:48.627450980392155\n",
      "Epoch: [2/20], Step: [80/292], Loss: 0.6903266906738281, Train Acc: 50.57647963105303, Validation Acc:52.450980392156865\n",
      "Epoch: [2/20], Step: [160/292], Loss: 0.6922510266304016, Train Acc: 50.86685455632419, Validation Acc:48.13725490196079\n",
      "Epoch: [2/20], Step: [240/292], Loss: 0.6945252418518066, Train Acc: 51.575710991544966, Validation Acc:51.76470588235294\n",
      "Epoch: [3/20], Step: [80/292], Loss: 0.7019426226615906, Train Acc: 50.64480314288154, Validation Acc:48.627450980392155\n",
      "Epoch: [3/20], Step: [160/292], Loss: 0.6683098673820496, Train Acc: 51.51592791869502, Validation Acc:48.72549019607843\n",
      "Epoch: [3/20], Step: [240/292], Loss: 0.6824702620506287, Train Acc: 52.404133572465625, Validation Acc:51.76470588235294\n",
      "Epoch: [4/20], Step: [80/292], Loss: 0.6361961960792542, Train Acc: 52.24186523187292, Validation Acc:49.11764705882353\n",
      "Epoch: [4/20], Step: [160/292], Loss: 0.6974996328353882, Train Acc: 52.60910410795115, Validation Acc:51.568627450980394\n",
      "Epoch: [4/20], Step: [240/292], Loss: 0.6446331739425659, Train Acc: 53.411905371936115, Validation Acc:52.35294117647059\n",
      "Epoch: [5/20], Step: [80/292], Loss: 0.6860182285308838, Train Acc: 53.83038688188573, Validation Acc:53.03921568627451\n",
      "Epoch: [5/20], Step: [160/292], Loss: 0.6279622316360474, Train Acc: 53.164232641557774, Validation Acc:49.21568627450981\n",
      "Epoch: [5/20], Step: [240/292], Loss: 0.6673965454101562, Train Acc: 53.92433171064993, Validation Acc:52.35294117647059\n",
      "Epoch: [6/20], Step: [80/292], Loss: 0.687807559967041, Train Acc: 54.1890853189854, Validation Acc:48.72549019607843\n",
      "Epoch: [6/20], Step: [160/292], Loss: 0.6192814111709595, Train Acc: 54.83815868135622, Validation Acc:53.431372549019606\n",
      "Epoch: [6/20], Step: [240/292], Loss: 0.6922487020492554, Train Acc: 55.598257750448376, Validation Acc:51.96078431372549\n",
      "Epoch: [7/20], Step: [80/292], Loss: 0.6603843569755554, Train Acc: 56.02527969937655, Validation Acc:49.6078431372549\n",
      "Epoch: [7/20], Step: [160/292], Loss: 0.6653845906257629, Train Acc: 57.74190793406781, Validation Acc:53.8235294117647\n",
      "Epoch: [7/20], Step: [240/292], Loss: 0.6245602369308472, Train Acc: 62.200017080877956, Validation Acc:58.03921568627451\n",
      "Epoch: [8/20], Step: [80/292], Loss: 0.4188169538974762, Train Acc: 83.46571013750106, Validation Acc:77.3529411764706\n",
      "Epoch: [8/20], Step: [160/292], Loss: 0.39827537536621094, Train Acc: 86.85626441199078, Validation Acc:80.09803921568627\n",
      "Epoch: [8/20], Step: [240/292], Loss: 0.33142128586769104, Train Acc: 91.96344692117175, Validation Acc:85.0\n",
      "Epoch: [9/20], Step: [80/292], Loss: 0.18399958312511444, Train Acc: 93.95336920317705, Validation Acc:83.52941176470588\n",
      "Epoch: [9/20], Step: [160/292], Loss: 0.3141031861305237, Train Acc: 96.24220684943207, Validation Acc:85.58823529411765\n",
      "Epoch: [9/20], Step: [240/292], Loss: 0.15300151705741882, Train Acc: 97.1901955760526, Validation Acc:87.05882352941177\n",
      "Epoch: [10/20], Step: [80/292], Loss: 0.03121996857225895, Train Acc: 97.63429840293792, Validation Acc:86.17647058823529\n",
      "Epoch: [10/20], Step: [160/292], Loss: 0.21640996634960175, Train Acc: 98.71039371423691, Validation Acc:87.94117647058823\n",
      "Epoch: [10/20], Step: [240/292], Loss: 0.07038644701242447, Train Acc: 99.15449654112221, Validation Acc:87.05882352941177\n",
      "Epoch: [11/20], Step: [80/292], Loss: 0.004872045014053583, Train Acc: 99.40216927150055, Validation Acc:87.6470588235294\n",
      "Epoch: [11/20], Step: [160/292], Loss: 0.008958162739872932, Train Acc: 99.41070971047913, Validation Acc:86.27450980392157\n",
      "Epoch: [11/20], Step: [240/292], Loss: 0.02082196995615959, Train Acc: 99.50465453924332, Validation Acc:88.33333333333333\n",
      "Epoch: [12/20], Step: [80/292], Loss: 0.011745134368538857, Train Acc: 99.70962507472885, Validation Acc:87.94117647058823\n",
      "Epoch: [12/20], Step: [160/292], Loss: 0.10847977548837662, Train Acc: 99.63276112392177, Validation Acc:87.25490196078431\n",
      "Epoch: [12/20], Step: [240/292], Loss: 0.029056202620267868, Train Acc: 99.59005892902896, Validation Acc:87.45098039215686\n",
      "Epoch: [13/20], Step: [80/292], Loss: 0.001389042823575437, Train Acc: 99.76940814757879, Validation Acc:87.45098039215686\n",
      "Epoch: [13/20], Step: [160/292], Loss: 0.004040366504341364, Train Acc: 99.78648902553591, Validation Acc:87.15686274509804\n",
      "Epoch: [13/20], Step: [240/292], Loss: 0.00148296938277781, Train Acc: 99.82065078145017, Validation Acc:87.05882352941177\n",
      "Epoch: [14/20], Step: [80/292], Loss: 0.0011270869290456176, Train Acc: 99.85481253736442, Validation Acc:86.66666666666667\n",
      "Epoch: [14/20], Step: [160/292], Loss: 0.004678084049373865, Train Acc: 99.37654795456487, Validation Acc:85.98039215686275\n",
      "Epoch: [14/20], Step: [240/292], Loss: 0.013522344641387463, Train Acc: 99.66692287983602, Validation Acc:86.66666666666667\n",
      "Epoch: [15/20], Step: [80/292], Loss: 0.002072212751954794, Train Acc: 99.50465453924332, Validation Acc:84.70588235294117\n",
      "Epoch: [15/20], Step: [160/292], Loss: 0.0010454648872837424, Train Acc: 99.82065078145017, Validation Acc:87.05882352941177\n",
      "Epoch: [15/20], Step: [240/292], Loss: 0.012019569054245949, Train Acc: 99.8377316594073, Validation Acc:86.47058823529412\n",
      "Epoch: [16/20], Step: [80/292], Loss: 0.006117739249020815, Train Acc: 99.84627209838585, Validation Acc:87.15686274509804\n",
      "Epoch: [16/20], Step: [160/292], Loss: 0.0015570350224152207, Train Acc: 99.60713980698608, Validation Acc:87.74509803921569\n",
      "Epoch: [16/20], Step: [240/292], Loss: 0.01944635435938835, Train Acc: 99.70962507472885, Validation Acc:86.66666666666667\n",
      "Epoch: [17/20], Step: [80/292], Loss: 0.0010078309569507837, Train Acc: 99.88043385430011, Validation Acc:87.84313725490196\n",
      "Epoch: [17/20], Step: [160/292], Loss: 0.005790296476334333, Train Acc: 99.7181655137074, Validation Acc:87.6470588235294\n",
      "Epoch: [17/20], Step: [240/292], Loss: 0.003799180733039975, Train Acc: 99.76086770860022, Validation Acc:88.43137254901961\n",
      "Epoch: [18/20], Step: [80/292], Loss: 0.005697386804968119, Train Acc: 99.84627209838585, Validation Acc:88.03921568627452\n",
      "Epoch: [18/20], Step: [160/292], Loss: 0.002046070294454694, Train Acc: 99.76940814757879, Validation Acc:87.3529411764706\n",
      "Epoch: [18/20], Step: [240/292], Loss: 0.0930214449763298, Train Acc: 99.74378683064309, Validation Acc:86.86274509803921\n",
      "Epoch: [19/20], Step: [80/292], Loss: 0.0012014716630801558, Train Acc: 99.9060551712358, Validation Acc:87.84313725490196\n",
      "Epoch: [19/20], Step: [160/292], Loss: 0.001349343336187303, Train Acc: 99.92313604919293, Validation Acc:87.15686274509804\n",
      "Epoch: [19/20], Step: [240/292], Loss: 0.0005954093066975474, Train Acc: 99.9060551712358, Validation Acc:87.54901960784314\n",
      "Epoch: [20/20], Step: [80/292], Loss: 0.0008420348167419434, Train Acc: 99.82919122042873, Validation Acc:87.3529411764706\n",
      "Epoch: [20/20], Step: [160/292], Loss: 0.0025243395939469337, Train Acc: 99.9060551712358, Validation Acc:86.47058823529412\n",
      "Epoch: [20/20], Step: [240/292], Loss: 0.0006151756970211864, Train Acc: 99.88043385430011, Validation Acc:87.54901960784314\n",
      "Maximum in checkpoint: 88.43137254901961\n"
     ]
    }
   ],
   "source": [
    "validation_acc_history = []\n",
    "stop_training = False\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        data_batch = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch) \n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % (BATCH_SIZE*2) == 0:\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            val_acc = test_model(validation_loader, model)\n",
    "            print('Epoch: [{0}/{1}], Step: [{2}/{3}], Loss: {4}, Train Acc: {5}, Validation Acc:{6}'.format( \n",
    "                   epoch+1, NUM_EPOCHS, i+1, len(imdb_train)//BATCH_SIZE, loss.data.item(), \n",
    "                   train_acc, val_acc))\n",
    "            validation_acc_history.append(val_acc)\n",
    "            \n",
    "        # TODO: Implement early stopping or model selection based on Validation Acc!!!\n",
    "print('Maximum in checkpoint: {}'.format(np.max(validation_acc_history)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Evaluate based on best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meetup",
   "language": "python",
   "name": "meetup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
